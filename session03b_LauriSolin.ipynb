{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import random\n",
    "from IPython.core.debugger import Pdb\n",
    "ipdb = Pdb()\n",
    "random.seed(a=None, version=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sample behaviour of an MDP\n",
    "\n",
    "Let's take (again) a look at Sutton & Barto example 4.1 gridworld. On each iteration start at every (non-terminating) state and sample actions in succeeding states by selecting them from uniform distribution (each action - up, down, left, right - is equally probable). Run the episode until terminal state is encountered. Collect statistics to calculate average number of steps needed before completion for each start state. Should this number match with something you have seen earlier in the exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "        \n",
    "        \n",
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "reward = -1 \n",
    "directions = ['up', 'right', 'down', 'left'] #probably not needed\n",
    "maxiters = 10000\n",
    "eps = 0.0000001\n",
    "k = 0 # \"memory counter\" of iterations inside the for loop, note that for loop i-variable is regular loop variable\n",
    "\n",
    "rows = 4\n",
    "cols = 4\n",
    "\n",
    "stepsMatrix = np.zeros((rows_count, columns_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def isTerminal(r,c):      #helper function to check if terminal state or regular state\n",
    "    global rows_count, columns_count\n",
    "    if r == 0 and c == 0: #im a bit too lazy to check otherwise the iteration boundaries        \n",
    "        return True       #so that this helper function is a quick way to exclude computations\n",
    "    if r == rows_count-1 and c == columns_count-1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def getValue(row, col):    #helper func, get state value\n",
    "    global V\n",
    "    if row == -1: row =0   #if you bump into wall, you bounce back\n",
    "    elif row == 4: row = 3\n",
    "    if col == -1: col = 0\n",
    "    elif col == 4: col =3\n",
    "        \n",
    "    return V[row,col]\n",
    "\n",
    "def getState(row,col):\n",
    "    if row == -1: row =0   #helper func for the exercise:1\n",
    "    elif row == 4: row = 3\n",
    "    if col == -1: col = 0\n",
    "    elif col == 4: col =3\n",
    "    return row, col\n",
    "\n",
    "\n",
    "def makeEpisode(r,c):  #helper func for the exercise:1\n",
    "## return the count of steps ??\n",
    "#by definition, you should always start from non-terminal state, so\n",
    "#by minimum, you need at least one action to get to terminal state\n",
    "    stateWasTerm = False\n",
    "    stepsTaken = 0\n",
    "    curR = r\n",
    "    curC = c\n",
    "    while not stateWasTerm:\n",
    "\n",
    "        act = random.randint(0,3)\n",
    "        if act == 0: ##up\n",
    "            curR-=1\n",
    "        elif act == 1: ##right\n",
    "            curC+=1\n",
    "        elif act == 2: ## down\n",
    "            curR+=1\n",
    "        else:##left\n",
    "            curC-=1\n",
    "        stepsTaken +=1\n",
    "        curR,curC = getState(curR,curC)\n",
    "        stateWasTerm = isTerminal(curR,curC)\n",
    "    return stepsTaken\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     14.3512 19.8079 22.3901]\n",
      " [13.9732 18.0939 19.9505 19.7331]\n",
      " [19.7774 20.1481 17.9906 13.662 ]\n",
      " [22.186  20.1546 13.8741  0.    ]]\n"
     ]
    }
   ],
   "source": [
    "for n in range(maxiters):\n",
    "    \n",
    "    for r in range(rows_count): ##for all states\n",
    "        for c in range(columns_count):\n",
    "            if isTerminal(r, c): ##if was terminal states => nothing\n",
    "                continue\n",
    "            else:               ##else  => do useful things\n",
    "                episodeSteps = makeEpisode(r,c)\n",
    "                temp = stepsMatrix[r,c]\n",
    "                temp += episodeSteps\n",
    "                stepsMatrix[r,c] = temp\n",
    "               ##temp = stepsDict[(r,c)]\n",
    "                ##temp += episodeSteps\n",
    "                ##stepsDict[(r,c)] = temp\n",
    "\n",
    "#update the sumStepsTaken for each state, into the averages over episodes                \n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        stepsMatrix[r,c] /= maxiters\n",
    "\n",
    "print(stepsMatrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answer for monte carlo thingy\n",
    "\n",
    "* when computing for each state, ( (sum of episodicSteps) / episodes)\n",
    "* those values inside their proper places in the numpy matrix looks like they are close to the absolute value of (state-value function)\n",
    "* in regular gridworld the values were the same except they were negative, because in gridworld there was always reward = -1\n",
    "* NOTE!, maxiters was only 10k initially, becvause I kept it low, so that I could better find out what was happening later on in this notebook in the ex2 monte carlo, where I used episodeCount variable instaed to control the iterations. I kept the maxiters lower, so the notebook would run faster, and I kept episodeCount higher so I got more reliable monte carlo's, I think..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Monte Carlo state value function estimation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1 using first-visit or every-visit Monte Carlo policy evaluation (see for example page 92 of Sutton & Barto). Policy to be evaluated is the same as before; each action (up, down, left, right) is equally probable.  Action that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "Try out both exploring starts (see Sutton & Barto, p. 96) and fixed start points. Any difference?\n",
    "\n",
    "Take a look at the value function you get when you run the algorithm multiple times (with fixed # of iterations). Any observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE! I will only do every-visit monte carlo (fixed start_loc) because...\n",
    "*  I didn't undertstand how to do first visit monte carlo. The checking for the first-visit in the pseudocode didn't seem to ring any bells in my head..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): [], (0, 2): [], (0, 3): [], (1, 0): [], (1, 1): [], (1, 2): [], (1, 3): [], (2, 0): [], (2, 1): [], (2, 2): [], (2, 3): [], (3, 0): [], (3, 1): [], (3, 2): []}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"some initializations, for datastructures and variables\"\"\"\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "episodeCount = 100000*2\n",
    "reward = -1\n",
    "y = 1.0 #the gamma discount rate\n",
    "\n",
    "\n",
    "#use dictionary where key is stateTuple, \n",
    "#and value is stateReturnsList\n",
    "#after algorithm for monte carlo policy eval is done, \n",
    "#we can update the dict into good format for printing\n",
    "#and use numpy matrix\n",
    "returnsDict={} \n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        if not isTerminal(r,c):\n",
    "            returnsDict[(r,c)]=[]\n",
    "\n",
    "        \n",
    "print(returnsDict)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" montecarlo episode generation\n",
    "returns the episodelist\"\"\"\n",
    "def MCEpisode(r,c):\n",
    "    global reward\n",
    "    stateWasTerm = False\n",
    "    stepsTaken = 0\n",
    "    curR = r \n",
    "    curC = c\n",
    "    episodeList=[]\n",
    "    #visitedStates=[]\n",
    "    \n",
    "    while not stateWasTerm:\n",
    "\n",
    "        act = random.randint(0,3)\n",
    "        if act == 0: ##up\n",
    "            r-=1\n",
    "            act=\"U\"\n",
    "        elif act == 1: ##right\n",
    "            c+=1\n",
    "            act=\"R\"\n",
    "        elif act == 2: ## down\n",
    "            r+=1\n",
    "            act=\"D\"\n",
    "        else:##left\n",
    "            c-=1\n",
    "            act=\"L\"\n",
    "        stepsTaken +=1\n",
    "        \n",
    "        r,c = getState(r,c)\n",
    "        stateWasTerm = isTerminal(r,c)\n",
    "        episodeList.append( ((curR,curC), act, reward) )\n",
    "        #visitedStates.append( (curR,curC)  )\n",
    "        if not stateWasTerm:\n",
    "            #episodeList.append( ((curR,curC), act, reward) )\n",
    "            curR = r\n",
    "            curC = c\n",
    "        \n",
    "        \n",
    "    return episodeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomStartState():\n",
    "    illegalState = True\n",
    "\n",
    "    \n",
    "    while illegalState:\n",
    "        r = random.randint(0,3)\n",
    "        c = random.randint(0,3)\n",
    "        \n",
    "        if (r==0 and c==0) or (r==3 and c==3):\n",
    "            illegalState = True\n",
    "        else:\n",
    "            illegalState= False\n",
    "    return r,c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"every-visit Monte Carlo with fixed starting state in the s(1,1) state\n",
    "or alternatively start from random state (sR,sC)\"\"\"\n",
    "kakka=0 #for debug breakpoints only!\n",
    "\n",
    "for n in range(1, episodeCount+1):\n",
    "    #ipdb.set_trace() # debugging starts here\n",
    "    epList = MCEpisode(1,1)\n",
    "    G = 0\n",
    "    for t in reversed( range( len(epList) )):\n",
    "        \n",
    "        G = y*G + reward #NOTE! reward is always same -1\n",
    "        S_t = epList[t][0] #get the state only, from tuple\n",
    "        willAppend = True\n",
    "        if(willAppend):\n",
    "            returnsDict[S_t].append(G)\n",
    "        \n",
    "    \n",
    "kakka = 3 #for debug breakpoints only!\n",
    "\n",
    "#print(returnsDict)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed start_loc Monte Carlo, every-visit\n",
      "\n",
      "[[  0.         -13.94935302 -19.97969824 -22.06874196]\n",
      " [-14.12209103 -18.04649906 -19.96745157 -19.98042336]\n",
      " [-20.15933997 -20.07762604 -18.09017819 -14.06051972]\n",
      " [-22.05534641 -20.13186602 -14.20269952   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"I think the average was for every-visit MC as follows:\n",
    "for each state\n",
    "    get the list of returns and sum the elements together,\n",
    "    divide that sum, by the amount of elements that were in the list\n",
    "\n",
    "This will not work, IF BY VERY UNLUCKY EVENT, we will have missesd entirely some state, \n",
    "across all the episodes... because...\n",
    "because then... we would get DivisionByZeroError...\"\"\"\n",
    "\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        if not isTerminal(r,c):\n",
    "        #divisor = len(returnsDict[(r,c)])\n",
    "        #if divisor == 0:\n",
    "        #    divisor+=1\n",
    "            V[r,c] = sum(returnsDict[(r,c)]) / ( len(returnsDict[(r,c)]) ) # adding +1 to prevent divisionByZero, not all states \n",
    "                                                     # were visited apparently...???\n",
    "\"\"\"there used to be a divisionByZero error at this stage, but \n",
    "most likely it was caused by iterating over the terminalStates somehow,\n",
    "\n",
    "theoretically it would be possible to have divisionbyZero error also in non-terminal state\n",
    "but that would have required that across all episodes, there was some state in 14 non-terminal-states that\n",
    "was never visited...\n",
    "\n",
    "That could have happened if episodeCount is low, and then you never visit some particular state,\n",
    "which leaves that state's own list empty, which causes the length of that list to be zero =>\n",
    "divisionByZeroError\n",
    "\n",
    "it will be unlikely to happen if episodesCount is high for Monte-Carlo\"\"\"\n",
    "print(\"fixed start_loc Monte Carlo, every-visit\\n\")            \n",
    "print(V)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results for  every-visit Monte-Carlo (fixed start_loc)\n",
    "* results look similar to the state value function from the chapter 4 exercises with MDP style algorithms for state value function for randomwalk policy. Not, quite equal, but good enough I hope?\n",
    "* it might be possible that the value function is little bit skewed towards favouring the upper left corner better\n",
    "* this is because starting state was fixed to be (1,1) because Sutton&Barto did not specify in the pseudocode if the starting state  should be randomized or not, or if you should vary starting state somehow...\n",
    "\n",
    "* so that if upper left corner has bigger state values (closer towards zero) then that might be cause of the favoritism of starting loc == (1,1)\n",
    "\n",
    "* because the grid was 4x4, it will be impossible to choose centerpoint that is equal for all sides, because it is missing the \"center element\" which would equally divide the array in length and width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to do every-visit Monte-Carlo (exploring starts)\n",
    "* that's the million dollar question isn't it\n",
    "* I will attempt to do it now\n",
    "* by definition it was so that in exploring starts...\n",
    "* every state-action pair has to be visited very often (equally often???)\n",
    "\n",
    "\n",
    "#### Monte Carlo ES (Exploring Starts), for estimating policy Pi ~~ Pi* (p. 99 )\n",
    "\n",
    "* in the pseudocode, it was mentioned that it will estimate policy into the optimal policy eventually.'\n",
    "* it also used Q(s,a) instead of V(s)\n",
    "* but, the crucial difference seemed to be that in Exploring starts you select starting state randomly, and starting action randomly, with non-zero probabilities (I wonder, if it means you can select the starting state and starting action with the original equiprobable thingy, such as 1/14 chance  of non-terminating random state, and 1/4 action)\n",
    "\n",
    "* I think we could still attempt to do the exploring starts case, but simply use it for the V(s) and then just randomize starting state.\n",
    "\n",
    "* the expectations is that  for V(s) will be that with the same amount of episodes run, the Exploring starts V(s) should be in equilibrium just like it was in chapter 4 MDP exercises V(s). This is in stark difference with fixed start Monte Carlo which was slanted to upper left corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "{(0, 1): [], (0, 2): [], (0, 3): [], (1, 0): [], (1, 1): [], (1, 2): [], (1, 3): [], (2, 0): [], (2, 1): [], (2, 2): [], (2, 3): [], (3, 0): [], (3, 1): [], (3, 2): []}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"resetting the variables and datastructuires for new round\"\"\"\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "returnsDict={} \n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        if not isTerminal(r,c):\n",
    "            returnsDict[(r,c)]=[]\n",
    "\n",
    "print(V)\n",
    "print(returnsDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"every-visit Monte Carlo with start from random state (sR,sC)\"\"\"\n",
    "kakka=0 #for debug breakpoints only!\n",
    "\n",
    "for n in range(1, episodeCount+1):\n",
    "    #ipdb.set_trace() # debugging starts here\n",
    "    sR, sC = getRandomStartState()\n",
    "    epList = MCEpisode(sR,sC)\n",
    "    G = 0\n",
    "    for t in reversed( range( len(epList) )):\n",
    "        \n",
    "        G = y*G + reward #NOTE! reward is always same -1\n",
    "        S_t = epList[t][0] #get the state only, from tuple\n",
    "        willAppend = True\n",
    "        if(willAppend):\n",
    "            returnsDict[S_t].append(G)\n",
    "        \n",
    "    \n",
    "kakka = 3 #for debug breakpoints only!\n",
    "\n",
    "#print(returnsDict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploring starts Monte Carlo, every-visit\n",
      "\n",
      "[[  0.         -13.96630385 -19.9212714  -21.90385924]\n",
      " [-13.91160224 -17.99611573 -20.03500858 -20.0187521 ]\n",
      " [-19.9440678  -19.97834431 -18.05784158 -13.99097402]\n",
      " [-21.94451288 -19.96657264 -14.05662013   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        if not isTerminal(r,c):\n",
    "        #divisor = len(returnsDict[(r,c)])\n",
    "        #if divisor == 0:\n",
    "        #    divisor+=1\n",
    "            V[r,c] = sum(returnsDict[(r,c)]) / ( len(returnsDict[(r,c)]) ) # adding +1 to prevent divisionByZero, not all states \n",
    "                                                     # were visited apparently...???\n",
    "\"\"\"there used to be a divisionByZero error at this stage, but \n",
    "most likely it was caused by iterating over the terminalStates somehow,\n",
    "\n",
    "theoretically it would be possible to have divisionbyZero error also in non-terminal state\n",
    "but that would have required that across all episodes, there was some state in 14 non-terminal-states that\n",
    "was never visited...\n",
    "\n",
    "That could have happened if episodesCouint is low, and then you never visit some particular state,\n",
    "which leaves that state's own list empty, which causes the length of that list to be zero =>\n",
    "divisionByZeroError\n",
    "\n",
    "it will be unlikely to happen if episodesCount is high for Monte-Carlo\"\"\"\n",
    "print(\"exploring starts Monte Carlo, every-visit\\n\")           \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results for every-visit Monte-Carlo (exploring starts)\n",
    "\n",
    "* I decided to turn up the episodecount to 500k\n",
    "* this had the effect of actually removing the skewed value function for fixed start_loc V(s) MC\n",
    "* also the results for randomized start loc V(s) MC were quite similar.\n",
    "* no big differences between those V(s) in my opinion\n",
    "\n",
    "* 500k episodecouint is very laggy though, for python to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3*: Monte Carlo action value function estimation\n",
    "\n",
    "Use the same idea as in exercise 2 to estimate q function.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4*: Monte Carlo control\n",
    "\n",
    "Compute the optimal policy for the 4x4 gridworld example. Start with random policy. Consider the epsilon adjustment schedule - can it in practise be 1/k, or is something more conservative better? Can you think of any other tricks to manage the noisiness of MC?\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
