{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 2\n",
    "\n",
    "Note that you don't have to use the functions / other code in the cells below. They are there just in case you need inspiration to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Iterative policy evaluation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1. Policy is assumed to be random, ie. each of the four directions are equally likely. Movement that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "When norm of the difference between new V and the old one is less than eps, stop iteration.\n",
    "\n",
    "Compare needed number of iterations between synchronous (sweep over all states, and update value function after the sweep) and asynchronous (use always the latest values) update of state-value function.\n",
    "\n",
    "Note that numpy tensor assignment does not create a copy. You might want to use .copy() method to avoid sharing a reference to the same array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    " ## How to do exercise 1 policy evaluation\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I'm going to borrow a phrase from teacher's playbook. \n",
    "#### Isn't it so that...\n",
    "\n",
    "\n",
    "Original formula was $$ V_{k+1} =  \\sum \\limits_{a} { \\pi(a|s) } * \\sum \\limits_{s',r} { [p( s', r | s, a )* (r + \\gamma * V(s') ) ]} $$\n",
    "\n",
    "We should be able to make some nice simplification into this monster formula as follows\n",
    "\n",
    "* substitute practical values into formula\n",
    "* use algebraic simplification\n",
    "\n",
    "input practical values because we know the following values for gridworld\n",
    "in these variables are in fact constants in the gridworld random walk policy...\n",
    "\n",
    "\n",
    "Especially because it was random walk policy, then isn't it so that the probability p(s', r | s,a ) must be  0.25, because it was random walk, and you could end up in any of the four possible newStates with equal probability\n",
    "because it was said in the slides as follows:\n",
    "\n",
    "\"Probability of ending up in\n",
    "state s’ when taking action a\n",
    "in state s. When multiplied\n",
    "with the value of state s’ and\n",
    "summed up, this gives the\n",
    "expected value. \"\n",
    "\n",
    "Also isn't it so that the sum_for_all_actions( pi(a|s) ) = 1, by definition...\n",
    "\n",
    "Algebraically, isn't it also so... that if you have calculation like (0.33 + 0.32 + 0.17 +0.18) * (w+x+y+z) = 1*(w+x+y+z)\n",
    "\n",
    "$$ \\gamma = 1 \\\\ \n",
    "reward = r= -1  \\\\  \n",
    "p( s', r | s, a ) = 0.25  \\\\ \n",
    "\\sum \\limits_{a} { \\pi(a|s) } = 0.25 + 0.25 + 0.25 + 0.25 = 1 $$\n",
    "\n",
    "So that the simplifications can be made into a simpler formula such as follows:\n",
    "\n",
    "\n",
    "$$ V_{k+1} = 0,25 * \\sum \\limits_{s'} { (  -1 + V(s') ) } $$\n",
    "\n",
    "* so that you have to substitute the constant values into the original formula\n",
    "* so that you move the p(s',r|s,a) outside the sum sign, because it is constant factor and it will be multiplied it is allowed\n",
    "* then you are left with the sum_for_all_newStates( -1 + V(s')  ) under the influence of the sum formula. Also it will be so, that the sum formula will run for 1,2,3,4 newStates always (up right down left), but some of those newStates might be the same as oldState because you bump into the wall\n",
    "* the minus one, is simply the constant value reward always -1\n",
    "* for the cases in the summation, where you will iterate into the wall, and get bounced back from the newState (s') then it will simply be that way that the newState will be the same as the originalState (s)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "iteration 300\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "stepcost = -1 #probably not needed\n",
    "directions = ['up', 'right', 'down', 'left'] #probably not needed\n",
    "maxiters = 1000000\n",
    "eps = 0.0000001\n",
    "k = 0 # \"memory counter\" of iterations inside the for loop, note that for loop i-variable is regular loop variable\n",
    "\n",
    "def isTerminal(r,c, maxR, maxC):      #helper function to check if terminal state or regular state\n",
    "    if r == 0 and c == 0: #im a bit too lazy to check otherwise the iteration boundaries        \n",
    "        return True       #so that this helper function is a quick way to exclude computations\n",
    "    if r == maxR-1 and c == maxC-1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def getValue(row, col, V):    #you only need this getter function to access the oldArrayV\n",
    "    if row == -1: row =0   #in the algorithm\n",
    "    elif row == 4: row = 3\n",
    "    if col == -1: col = 0\n",
    "    elif col == 4: col =3\n",
    "        \n",
    "    return V[row,col]\n",
    "\n",
    "\"\"\"synchronous case\"\"\"\n",
    "\"\"\"Iterative Policy Evaluation algorithm, for estimating V ~ v_pi\n",
    "using randomWalk policy\"\"\"\n",
    "#basically you choose maxiters = k iterations\n",
    "#as it was described in the gridworld example K= iteration count\n",
    "# if maxiters =0 => results in 0 board\n",
    "# if maxiters =1 => results in -1 board, except terminal state\n",
    "# if maxiters =2 => results in that changed board\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(maxiters):\n",
    "    delta = 0\n",
    "    for row in range(4):    #for all states\n",
    "        for column in range(4):\n",
    "            \n",
    "            if(isTerminal(row, column, rows_count, columns_count)): # in case of terminal state\n",
    "                newVal = 0\n",
    "            else:\n",
    "                oldVal = getValue(row, column, V) # in case of regular state\n",
    "                newVal = 0.25 * (-4 + \\\n",
    "                                getValue(row, column-1, V) + \\\n",
    "                                getValue(row+1, column, V) + \\\n",
    "                                getValue(row, column+1, V) + \\\n",
    "                                getValue(row-1, column, V)  )\n",
    "                V_new[row, column]= newVal   # update the values into the newArrayV\n",
    "                delta = max(delta, abs(oldVal - newVal))\n",
    "    V = V_new.copy() #update old array into previouosly calculated newVersion\n",
    "    k+=1 # use k-variable as the \"memory counter\" so that you can keep the iteration count in memory after for loop\n",
    "    if delta < eps: #epsilon convergence condition for early break\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "print(\"iteration\", k)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)\n",
    "    \n",
    "print(\"iteration\", k)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 193\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"asynchronous case\"\"\"\n",
    "\"\"\"Iterative Policy Evaluation algorithm, for estimating V ~ v_pi\n",
    "using randomWalk policy\n",
    "\n",
    "NOTE::!!! USE THE OLDARRAY V ONLY!!!\"\"\"\n",
    "\n",
    "V = np.zeros((rows_count, columns_count))    #reset the arrays\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "\n",
    "\n",
    "\n",
    "#basically you choose maxiters = k iterations\n",
    "#as it was described in the gridworld example K= iteration count\n",
    "# if maxiters =0 => results in 0 board\n",
    "# if maxiters =1 => results in -1 board, except terminal state\n",
    "# if maxiters =2 => results in that changed board\n",
    "k = 0 # \"memory counter\" of iterations inside the for loop, note that for loop i-variable is regular loop variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(maxiters):\n",
    "    delta = 0\n",
    "    for row in range(4):    #for all states\n",
    "        for column in range(4):\n",
    "            \n",
    "            if(isTerminal(row, column, rows_count, columns_count)): # in case of terminal state\n",
    "                newVal = 0\n",
    "            else:\n",
    "                oldVal = getValue(row, column, V) # in case of regular state\n",
    "                newVal = 0.25 * (-4 + \\\n",
    "                                getValue(row, column-1, V) + \\\n",
    "                                getValue(row+1, column, V) + \\\n",
    "                                getValue(row, column+1, V) + \\\n",
    "                                getValue(row-1, column, V)  )\n",
    "                V[row, column]= newVal   # update the values into the oldArrayV, we are only using that old array only!!!\n",
    "                delta = max(delta, abs(oldVal - newVal))\n",
    "    #V = V_new.copy() #update old array into previouosly calculated newVersion\n",
    "    k+=1 # use k-variable as the \"memory counter\" so that you can keep the iteration count in memory after for loop\n",
    "    if delta < eps: #epsilon convergence condition for early break\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "print(\"iteration\", k)\n",
    "with np.printoptions(precision=3):\n",
    "    print(V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing asynch vs synch policy evaluation algorithms\n",
    "\n",
    "* it appears true that in this case, at least, the asynch version was converging more rapidly, quite a bit faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Greedy policy. \n",
    "\n",
    "Based on the state-value function computed in exercise 1, print out deterministic greedy policy function. Is the policy generated also optimal one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do exercise 2 greedy policy\n",
    "\n",
    "* based on my reading of the book, the policyEval+Greedy should provide us the optimal policy assuming you let the algorithm run its own course and and when it's done\n",
    "\n",
    "* if you only make 1 iteration of the full algorithm it's not guaranteed to converge in that short amount of iterations\n",
    "\n",
    "\n",
    "## how to print the policy (probably use a table like the gridworld example shows)\n",
    "\n",
    "* I think the good way to represent policy function\n",
    "   (if that's  even a real thing, and not fake news)\n",
    "   would be to have matrix of int, which are basically the direction arrows\n",
    "   to follow in the policy. This is like the arrows that are in the gridworld\n",
    "   policy map.\n",
    "\n",
    "\n",
    "* int in matrix should initialy be random directional == 15 \n",
    "  then, you could represent that with a start or something\n",
    "\n",
    "* then you print them you can print nice unicode arrow signs, with regular directions (actions) \n",
    "\n",
    "* Another choice might be simply numpy matrix of unicode chars, then you coiuld just store the arrows inside or some char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"also collect the old State Value fucnt into safe storage actually, \n",
    "so we can later use deterministic greedy policy. I think we should prepare\n",
    "to do the greedy optimization\"\"\"\n",
    "\n",
    "stateValFunc = V.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"here are some helper function definitions, which\n",
    "should help in the formulation of the policyEvaluate + greedy so we dont get bloated code\n",
    "at least too much\"\"\"\n",
    "\n",
    "def asynchPolEval(maxiters, rows, cols, eps, V, pol, rew):\n",
    "    V = V.copy() ## for simplicity sakes, work with local copy of arrayV, and return arrayV afterwards\n",
    "    Pol = pol.copy()\n",
    "    Rew = rew[:]\n",
    "    \n",
    "    for i in range(maxiters): ## iterate policy evaluation\n",
    "        delta = 0\n",
    "        \n",
    "        for r in range(rows):    ## for all states\n",
    "            for c in range(cols):\n",
    "            \n",
    "                if(isTerminal(r, c, rows, cols)): ## in case of terminal state\n",
    "                    continue\n",
    "                else:\n",
    "                    oldVal = getValue(r, c, V) ## in case of regular state\n",
    "                    if Pol[r,c] == 'A':        ## if we still had the random walk policy\n",
    "                        newVal = 0.25 * (sum(Rew) + getValue(r, c-1, V) + \\\n",
    "                                         getValue(r+1, c, V) + \\\n",
    "                                         getValue(r, c+1, V) + \\\n",
    "                                         getValue(r-1, c, V)  )\n",
    "                    else: ## we have deterministic policy instead                    \n",
    "                        curPol = Pol[r,c]\n",
    "                        if curPol == \"U\": probs = [1,0,0,0] ## up, right, down left\n",
    "                        elif curPol == \"R\":probs = [0,1,0,0] ## up, right, down left\n",
    "                        elif curPol == \"D\":probs = [0,0,1,0] ## up, right, down left\n",
    "                        elif curPol == \"L\":probs = [0,0,0,1] ## up, right, down left\n",
    "                        \n",
    "\n",
    "                        newVal = (probs[0] * (  Rew[0] + getValue(r-1, c, V) )  ) + \\\n",
    "                                (probs[1] * (  Rew[1] + getValue(r, c+1, V) )  ) + \\\n",
    "                                (probs[2] * (  Rew[2] + getValue(r+1, c, V) )  ) + \\\n",
    "                                (probs[3] * (  Rew[3] + getValue(r, c-1, V) )  ) \n",
    "                        \n",
    "                    V[row, column]= newVal   ## update the values into the oldArrayV, we are only using that old array only!!!\n",
    "                    delta = max(delta, abs(oldVal - newVal))\n",
    "        if delta < eps: ## epsilon convergence condition for early break from iterate policy evaluation\n",
    "            return V\n",
    "            \n",
    "            \n",
    "            \n",
    "def synchPolEval(maxiters, rows, cols, eps, V, V_new, pol, rew):\n",
    "    V = V.copy() ## for simplicity sakes, work with local copy of arrayV, and return arrayV afterwards\n",
    "    V_new = V_new.copy()\n",
    "    Pol = pol.copy()\n",
    "    Rew = rew[:]\n",
    "    \n",
    "    \n",
    "    for i in range(maxiters):\n",
    "        delta = 0\n",
    "        for r in range(rows):    #for all states\n",
    "            for c in range(cols):\n",
    "                if(isTerminal(r, c, rows, cols)): # in case of terminal state\n",
    "                    continue\n",
    "                else:\n",
    "                    oldVal = getValue(r, c, V) # in case of regular state\n",
    "                    if Pol[r,c] == 'A': ## if we still had the random walk policy\n",
    "                        newVal = 0.25 * (sum(Rew) + getValue(r, c-1, V) + \\\n",
    "                                         getValue(r+1, c, V) + \\\n",
    "                                         getValue(r, c+1, V) + \\\n",
    "                                         getValue(r-1, c, V)  )                    \n",
    "                    else: ## we have deterministic policy instead                    \n",
    "                        curPol = Pol[r,c]\n",
    "                        if curPol == \"U\": probs = [1,0,0,0] ## up, right, down left\n",
    "                        elif curPol == \"R\":probs = [0,1,0,0] ## up, right, down left\n",
    "                        elif curPol == \"D\":probs = [0,0,1,0] ## up, right, down left\n",
    "                        elif curPol == \"L\":probs = [0,0,0,1] ## up, right, down left\n",
    "                        \n",
    "                        newVal = (probs[0] * (  Rew[0] + getValue(r-1, c, V) )  ) + \\\n",
    "                                (probs[1] * (  Rew[1] + getValue(r, c+1, V) )  ) + \\\n",
    "                                (probs[2] * (  Rew[2] + getValue(r+1, c, V) )  ) + \\\n",
    "                                (probs[3] * (  Rew[3] + getValue(r, c-1, V) )  )  \n",
    "                    \n",
    "                    \n",
    "                    V_new[r, c]= newVal   # update the values into the newArrayV\n",
    "                    delta = max(delta, abs(oldVal - newVal))\n",
    "        V = (V_new.copy()) #update old array into previouosly calculated newVersion\n",
    "        if delta < eps: #epsilon convergence condition for early break\n",
    "            return V\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['T' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'T']]\n",
      "A stands for any direction\n",
      "R is for right, D stands for down etc...\n",
      "T is for terminalNode, no action needed\n",
      "im afraid that I didnt find a good way to include partial actions into single cell \n",
      " such as  'up-right', because theorereically isnt it so... that  \n",
      " you will have 16 different combos for those actions in a single cell in the policies table \n",
      " this will be so, because you have basically 4bit binary number which is the possible actions in \n",
      " a certain state, but it can be any one of them (such that e.g. 0 == terminal node, \n",
      " and 14 == up-right-down, 15 == up-right-down-left, 12 == up-right )\n"
     ]
    }
   ],
   "source": [
    "\"\"\"initial values for actions are A for any, which means all directions \n",
    "are the action in that state, in other words, it means exactly that we have random walk policy at start\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "policies = np.array([ ['T','A','A','A'], \n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','T'] ])\n",
    "\n",
    "\n",
    "print(policies)\n",
    "print('A stands for any direction')\n",
    "print('R is for right, D stands for down etc...')\n",
    "print('T is for terminalNode, no action needed')\n",
    "\n",
    "\n",
    "\n",
    "print(\"im afraid that I didnt find a good way to include partial actions into single cell \\n \\\n",
    "such as  'up-right', because theorereically isnt it so... that  \\n \\\n",
    "you will have 16 different combos for those actions in a single cell in the policies table \\n \\\n",
    "this will be so, because you have basically 4bit binary number which is the possible actions in \\n \\\n",
    "a certain state, but it can be any one of them (such that e.g. 0 == terminal node, \\n \\\n",
    "and 14 == up-right-down, 15 == up-right-down-left, 12 == up-right )\")\n",
    "\n",
    "\n",
    "def getAction(row, col, policies):    #you only need this getter function to access \n",
    "    if row == -1: row =0   #policies array\n",
    "    elif row == 4: row = 3\n",
    "    if col == -1: col = 0\n",
    "    elif col == 4: col =3\n",
    "        \n",
    "    return policies[row,col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original Value function was \n",
      " [[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "iterations done, k ==  1\n",
      "ending value function was \n",
      " [[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "policy was \n",
      " [['T' 'L' 'L' 'L']\n",
      " ['U' 'L' 'L' 'D']\n",
      " ['U' 'U' 'D' 'D']\n",
      " ['U' 'R' 'R' 'T']]\n",
      "\n",
      "as you can see, I only bothered with the case of single decision actions\n",
      "the book describes some of those actions as being \"coin  toss between up and right\", but this policies table with \n",
      " deterministic decisions should also be a correct optimal policy, but it contains only the single deterministic choices \n",
      " (which are optimal, but not necessarily complete as I just described)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"make exactly one iteration of policyEvaluate + GreedyOpt algorithm\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "V0 = stateValFunc.copy()    ## initialize to correct initial state val funct,                            \n",
    "V1 = stateValFunc.copy()  ## based on the randomwalk policy state val function\n",
    "\n",
    "actionStringDict = {0:'U', 1:'R', 2:'D', 3:'L'} ##check what values to put into new policy, to replace old ones\n",
    "rewards = [-1, -1, -1, -1]\n",
    "\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(\"original Value function was \\n\",V0)\n",
    "\n",
    "\n",
    "\"\"\"greedy policy\"\"\"\n",
    "keepOpt = True\n",
    "\n",
    "k = 0 ## memory counter\n",
    "\n",
    "## only make one iteration with the policyEval+GreedyOpt algorithm, not guaranteed to be the best with \n",
    "## iterations count being this small\n",
    "## However, I think in this case it might be enough to converge quite immediately\n",
    "## also, if you change the main outer while loop condition to be while(keepOpt), then it \n",
    "## should give optimal policy in end\n",
    "\n",
    "\n",
    "while k<1:\n",
    "    k += 1 \n",
    "    V0 = synchPolEval(maxiters, 4, 4, eps, V0, V1, policies, rewards)\n",
    "    V1 = V0.copy() ## originally synchPolEval had twin return values, but I changed it somewhere...\n",
    "    policyStable = True ## so that we have to remember to update V1 also...\n",
    "    \n",
    "    for r in range(4):\n",
    "        for c in range(4): #for all states\n",
    "            \n",
    "            oldAct = policies[r,c] ## handle terminal node, in policies, nothing needs to be done, in first terminal,\n",
    "            if oldAct == 'T':      ##  and last terminal must end\n",
    "                continue\n",
    "                \n",
    "            else:    ##regular node    \n",
    "                tempList = [getValue(r-1, c, V0) + rewards[0], \n",
    "                             getValue(r, c+1, V0) + rewards[1], \n",
    "                             getValue(r+1, c, V0) + rewards[2], \n",
    "                             getValue(r, c-1, V0) + rewards[3]]  \n",
    "          \n",
    "    ## we already have list of rewards, list of newStates, for each action possible\n",
    "    ## I think that we can basically disregard the probability portion of the formula if we have deterministic greedy\n",
    "    ## because that p(s',r|s,a) will be either zero or one, depending on what the newState s' will be\n",
    "    ## such that... it will be physically impossible to have action == right, but the newState s' will be the upper state\n",
    "    ## the corresponding action will always make corresponding newState, and nothing else but that...\n",
    "    \n",
    "    ## so... basically we just compute list of values for [up,right,down,left] directions, and\n",
    "    ## each value will be ==  (r + V(s'))\n",
    "    ## from list, find maxVal and get that index\n",
    "    ## use that index to search into actionStringDict, which contains string values \"u\", \"r\", \"d\", \"l\"\n",
    "    ## update policy\n",
    "                \n",
    "                tempMax = max(tempList)    ##get best value from newStates\n",
    "                for n in range(len(tempList)): ## find what index it was\n",
    "                    if tempMax == tempList[n]:\n",
    "                        maxInd = n\n",
    "                        break\n",
    "                        \n",
    "                maxChar = actionStringDict[maxInd] ##use the index to access into correct string from the dict\n",
    "                newAct = maxChar\n",
    "                policies[r, c] = newAct    \n",
    "                     \n",
    "                if oldAct != newAct:\n",
    "                    policyStable = False\n",
    "    if policyStable:\n",
    "        keepOpt = False\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"iterations done, k == \",k)\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(\"ending value function was \\n\",V0)\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print(\"policy was \\n\",policies)\n",
    "    \n",
    "print('')\n",
    "print('as you can see, I only bothered with the case of single decision actions')\n",
    "print('the book describes some of those actions as being \"up or right\", but this policies table with \\n \\\n",
    "deterministic decisions should also be a correct optimal policy, but it contains only the single deterministic choices \\n \\\n",
    "(which are optimal, but not necessarily complete as I just described)')\n",
    "\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in conclusion, by eyeballing this solution for a policy (policyTable)\n",
    "\n",
    "* it is an optimal solution, judging by eyeball and loking at my own policy table and the book example policy table\n",
    "* it is not necessarily the unique  solution(???) \n",
    "* I left out the partial choice actions from the policy table\n",
    "  such as \"up-right\" and \"down-left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Value function and policy in modified gridworld.\n",
    "\n",
    "Change the definition of the exercise 1 gridworld by assigning a cost of -8 to movement in \"up\" direction. Compute the value function and greedy policy based on the value function. Is the greedy policy optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to solve ex3 greedyOpt, basic gridworld, but modified rewards\n",
    "\n",
    "* I had to do a fair bit of re-coding for my policy evaluation algorithm, but it appears I managed to do that re-coding correctly in the asynchPolEval, and synchPolEval functions\n",
    "\n",
    "\n",
    "* re-coding had to be done, because I had just hardcoded the policy evaluation to only use the randomwalk policy and only use the hardcoded rewards, and the randomwalk probability of choosing action (0.25 equiprobable).\n",
    "\n",
    "\n",
    "* Sutton and Barto pseudocode was worse than useless, because it was incredibly misleading pseudocode, it would have been much better to write that pseudocode for algorithms in much smarter way of notation usage... Heck, even English language pseudocode would have been much more understandable!!!\n",
    "\n",
    "\n",
    "* the policy in the end  should be optimal policy, because Sutton and Barto claimed it will converge to the optimal policy eventually, assuming algorithm was coded correctly\n",
    "\n",
    "* we will do this exercise 3 properly such that we allow the algorithm to run its own course until it ends in optimal policy (when compared to exercise 2, we will not only iterate once, but allow it to iterate how many times it needs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting V0, V1 and policies, and initalize rewards \n",
      "\n",
      "[['T' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'T']]\n",
      "\n",
      "V0 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "V1 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "rewards \n",
      " [-8, -1, -1, -1]\n",
      "k ==  0\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "        ## reset for new attemp at ex3\n",
    "print('reseting V0, V1 and policies, and initalize rewards \\n')\n",
    "\n",
    "policies = np.array([ ['T','A','A','A'], \n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','T'] ])\n",
    "print(policies)\n",
    "            \n",
    "print('')\n",
    "V0 = np.zeros((rows_count, columns_count))    #reset the arrays\n",
    "V1 = np.zeros((rows_count, columns_count))\n",
    "\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print('V0',V0)\n",
    "print('')\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print('V1',V1)\n",
    "print('')\n",
    "\n",
    "rewards = [-8, -1, -1, -1] ## must be in order up,right,down,left\n",
    "print('rewards \\n',rewards)\n",
    "k = 0\n",
    "print('k == ',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations done, k ==  4\n",
      "ending value function was \n",
      " [[ 0. -1. -2. -3.]\n",
      " [-5. -4. -3. -2.]\n",
      " [-4. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "policy was \n",
      " [['T' 'L' 'L' 'D']\n",
      " ['R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'T']]\n",
      "\n",
      "as you can see, I only bothered with the case of single decision actions\n",
      "the book describes some of those actions as being \"coin  toss between up and right\", but this policies table with \n",
      " deterministic decisions should also be a correct optimal policy, but it contains only the single deterministic choices \n",
      " (which are optimal, but not necessarily complete as I just described)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"greedy policy\"\"\"\n",
    "keepOpt = True\n",
    "\n",
    "while keepOpt:\n",
    "    k += 1 ## memory counter\n",
    "    V0 = synchPolEval(maxiters, 4, 4, eps, V0, V1, policies, rewards)\n",
    "    V1 = V0.copy() ## when this line was missing, it was subtle bug, because V1 would not have updated \n",
    "                   ## together with V0. Originally I had used twin return values in the function but I changed it somewhere\n",
    "    policyStable = True\n",
    "    \n",
    "    for r in range(4):\n",
    "        for c in range(4): #for all states\n",
    "            \n",
    "            oldAct = policies[r,c] ## handle terminal node, in policies, nothing needs to be done, in first terminal,\n",
    "            if oldAct == 'T':      ##  and last terminal must end\n",
    "                continue\n",
    "                \n",
    "            else:    ##regular node    \n",
    "                tempList = [getValue(r-1, c, V0) + rewards[0], \n",
    "                             getValue(r, c+1, V0) + rewards[1], \n",
    "                             getValue(r+1, c, V0) + rewards[2], \n",
    "                             getValue(r, c-1, V0) + rewards[3]]  \n",
    "          \n",
    "    ## we already have list of rewards, list of newStates, for each action possible\n",
    "    ## I think that we can basically disregard the probability portion of the formula if we have deterministic greedy\n",
    "    ## because that p(s',r|s,a) will be either zero or one, depending on what the newState s' will be\n",
    "    ## such that... it will be physically impossible to have action == right, but the newState s' will be the upper state\n",
    "    ## the corresponding action will always make corresponding newState, and nothing else but that...\n",
    "    \n",
    "    ## so... basically we just compute list of values for [up,right,down,left] directions, and\n",
    "    ## each value will be ==  (r + V(s'))\n",
    "    ## from list, find maxVal and get that index\n",
    "    ## use that index to search into actionStringDict, which contains string values \"u\", \"r\", \"d\", \"l\"\n",
    "    ## update policy\n",
    "                \n",
    "                tempMax = max(tempList)    ##get best value from newStates\n",
    "                for n in range(len(tempList)): ## find what index it was\n",
    "                    if tempMax == tempList[n]:\n",
    "                        maxInd = n\n",
    "                        break\n",
    "                        \n",
    "                maxChar = actionStringDict[maxInd] ##use the index to access into correct string from the dict\n",
    "                newAct = maxChar\n",
    "                policies[r, c] = newAct    \n",
    "                     \n",
    "                if oldAct != newAct:\n",
    "                    policyStable = False\n",
    "    if policyStable:\n",
    "        keepOpt = False\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"iterations done, k == \",k)\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(\"ending value function was \\n\",V0)\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print(\"policy was \\n\",policies)\n",
    "    \n",
    "print('')\n",
    "print('as you can see, I only bothered with the case of single decision actions')\n",
    "print('the book describes some of those actions as being \"up or right\", but this policies table with \\n \\\n",
    "deterministic decisions should also be a correct optimal policy, but it contains only the single deterministic choices \\n \\\n",
    "(which are optimal, but not necessarily complete as I just described)')\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results were such that, IT WAS INDEED OPTIMAL!\n",
    "\n",
    "* because I apparently coded algorithm correctly??!!! YAY! give me a cookie for reward! (or at least  points for the lab...)\n",
    "\n",
    "\n",
    "* We can see immediately from looking at the board  the following fact: \n",
    "  so we can see, such that if the reward for up == -8, then because the board size is only 4x4\n",
    "  then by definition we must have such that we NEVER choose up\n",
    "\n",
    "\n",
    "* also, looking at the policiesTable with eyeballs, it looks like it is an optimal solution, but not necessarily the unique one\n",
    "\n",
    "\n",
    "* from the first row, you should move to the upper left corner basically. But on first row last column you can go left or down\n",
    "\n",
    "\n",
    "* from the other rows, except the ones near the wall itself, it should be such that you can move down or right, but avoiding hitting the wall. Bascially from the other rows except the first one, you should keep sliding towards the bottom right corner as fast as possible with right or down moves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercise: Policy iteration\n",
    "\n",
    "Implement policy iteration, ie. create a policy with the help of the value function from previous policy and iterate until policy is stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do exercise Extra\n",
    "\n",
    "* I will just initialize randomwalk policy\n",
    "* I will use reward -1 for all except down = -8\n",
    "* I just noticed that in the exercises for the book (page. 82), it said that Sutton and Barto deliberately \n",
    "   coded a bug into their own pseudocode for policy iteration algorithm! The exercise said that\n",
    "   sometimes the pseudocode possible will get stuck between two equally good policies\n",
    "   \n",
    "* I didnt fix that bug yet, but usuallt it seems like if you start with randomwalk policy, then it\n",
    "  doesnt usually get stuck into forever loop\n",
    "  \n",
    "* it sounds like that bug could be inside the optimizer portion of the algorithm, because the policy\n",
    "    evaluation portion of algorithm was done with for loop so it is guaranteed to not get blocked \n",
    "    in forever loop\n",
    "\n",
    "\n",
    "* two possible fixes that I could think of might be  to have the optimizer also in forloop,\n",
    "\n",
    "* or alternatively the bug could be that there are two numerically equal actions, so that\n",
    "  they are  equal value... but the optimizer might keep bouncing between those... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting V0, V1 and policies, and initalize rewards \n",
      "\n",
      "[['T' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'T']]\n",
      "\n",
      "V0 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "V1 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "rewards \n",
      " [-1, -1, -8, -1]\n",
      "k ==  0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"reset variables for policyEval + greedyOpt\"\"\"\n",
    "\n",
    "       \n",
    "        ## reset for new attemp at ex3\n",
    "print('reseting V0, V1 and policies, and initalize rewards \\n')\n",
    "\n",
    "policies = np.array([ ['T','A','A','A'], \n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','T'] ])\n",
    "print(policies)\n",
    "            \n",
    "print('')\n",
    "V0 = np.zeros((rows_count, columns_count))    #reset the arrays\n",
    "V1 = np.zeros((rows_count, columns_count))\n",
    "\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print('V0',V0)\n",
    "print('')\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print('V1',V1)\n",
    "print('')\n",
    "\n",
    "rewards = [-1, -1, -8, -1] ## must be in order up,right,down,left\n",
    "print('rewards \\n',rewards)\n",
    "k = 0\n",
    "print('k == ',k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations done, k ==  4\n",
      "ending value function was \n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]\n",
      " [-3. -2. -1.  0.]]\n",
      "policy was \n",
      " [['T' 'L' 'L' 'L']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['U' 'R' 'R' 'T']]\n",
      "\n",
      "as you can see, I only bothered with the case of single decision actions\n",
      "the book describes some of those actions as being \"coin  toss between up and right\", but this policies table with \n",
      " deterministic decisions should also be a correct optimal policy, but it contains only the single deterministic choices \n",
      " (which are optimal, but not necessarily complete as I just described)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"greedy policy\"\"\"\n",
    "keepOpt = True\n",
    "\n",
    "while keepOpt:\n",
    "    k += 1 ## memory counter\n",
    "    V0 = synchPolEval(maxiters, 4, 4, eps, V0, V1, policies, rewards)\n",
    "    V1 = V0.copy() ## when this line was missing, it was subtle bug, because V1 would not have updated \n",
    "                   ## together with V0. Originally I had used twin return values in the function but I changed it somewhere\n",
    "    policyStable = True\n",
    "    \n",
    "    for r in range(4):\n",
    "        for c in range(4): #for all states\n",
    "            \n",
    "            oldAct = policies[r,c] ## handle terminal node, in policies, nothing needs to be done, in first terminal,\n",
    "            if oldAct == 'T':      ##  and last terminal must end\n",
    "                continue\n",
    "                \n",
    "            else:    ##regular node    \n",
    "                tempList = [getValue(r-1, c, V0) + rewards[0], \n",
    "                             getValue(r, c+1, V0) + rewards[1], \n",
    "                             getValue(r+1, c, V0) + rewards[2], \n",
    "                             getValue(r, c-1, V0) + rewards[3]]  \n",
    "          \n",
    "    ## we already have list of rewards, list of newStates, for each action possible\n",
    "    ## I think that we can basically disregard the probability portion of the formula if we have deterministic greedy\n",
    "    ## because that p(s',r|s,a) will be either zero or one, depending on what the newState s' will be\n",
    "    ## such that... it will be physically impossible to have action == right, but the newState s' will be the upper state\n",
    "    ## the corresponding action will always make corresponding newState, and nothing else but that...\n",
    "    \n",
    "    ## so... basically we just compute list of values for [up,right,down,left] directions, and\n",
    "    ## each value will be ==  (r + V(s'))\n",
    "    ## from list, find maxVal and get that index\n",
    "    ## use that index to search into actionStringDict, which contains string values \"u\", \"r\", \"d\", \"l\"\n",
    "    ## update policy\n",
    "                \n",
    "                tempMax = max(tempList)    ##get best value from newStates\n",
    "                for n in range(len(tempList)): ## find what index it was\n",
    "                    if tempMax == tempList[n]:\n",
    "                        maxInd = n\n",
    "                        break\n",
    "                        \n",
    "                maxChar = actionStringDict[maxInd] ##use the index to access into correct string from the dict\n",
    "                newAct = maxChar\n",
    "                policies[r, c] = newAct    \n",
    "                     \n",
    "                if oldAct != newAct:\n",
    "                    policyStable = False\n",
    "    if policyStable:\n",
    "        keepOpt = False\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"iterations done, k == \",k)\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print(\"ending value function was \\n\",V0)\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print(\"policy was \\n\",policies)\n",
    "    \n",
    "print('')\n",
    "print('as you can see, I only bothered with the case of single decision actions')\n",
    "print('the book describes some of those actions as being \"coin  toss between up and right\", but this policies table with \\n \\\n",
    "deterministic decisions should also be a correct optimal policy, but it contains only the single deterministic choices \\n \\\n",
    "(which are optimal, but not necessarily complete as I just described)')\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results were such that\n",
    "\n",
    "* it ended in optimal policy at this time\n",
    "* algorithm managed to avoid going down, which was what we wanted\n",
    "* algorithm went rightwards on the last row, when it was beneficial which is correct also\n",
    "* on first row the choices were correct leftwards because it's the straightest line possible\n",
    "* upwards and leftwards creep was to be expected otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Value iteration\n",
    "\n",
    "Solve the exercise 1 gridworld with value iteration algorithm. Solve also modified gridworld (cost of \"up\" movement = -4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to solve exercise4 value iteration algorithm\n",
    "* lets just test this basic algorithm first of all for the basic case\n",
    "* if it works for the basic case, where all rewards = -1\n",
    "* then it maybe works for other cases also ???!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting V0, V1 and policies, and initalize rewards \n",
      "\n",
      "[['T' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'T']]\n",
      "\n",
      "V0 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "V1 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "rewards \n",
      " [-1, -1, -1, -1]\n",
      "k ==  0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"reset variables for preparation for the value iteration algorithm\"\"\"\n",
    "\n",
    "       \n",
    "        ## reset for new attemp at ex3\n",
    "print('reseting V0, V1 and policies, and initalize rewards \\n')\n",
    "\n",
    "policies = np.array([ ['T','A','A','A'], \n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','T'] ])\n",
    "print(policies)\n",
    "            \n",
    "print('')\n",
    "V0 = np.zeros((rows_count, columns_count))    #reset the arrays\n",
    "V1 = np.zeros((rows_count, columns_count))\n",
    "\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print('V0',V0)\n",
    "print('')\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print('V1',V1)\n",
    "print('')\n",
    "\n",
    "rewards = [-1, -1, -1, -1] ## must be in order up,right,down,left\n",
    "print('rewards \\n',rewards)\n",
    "k = 0\n",
    "print('k == ',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k ==  1 : value func == \n",
      " \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "k ==  2 : value func == \n",
      " \n",
      " [[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "k ==  3 : value func == \n",
      " \n",
      " [[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "k ==  4 : value func == \n",
      " \n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "\n",
      " ending results were: \n",
      "\n",
      "value function was \n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "policy was \n",
      " [['T' 'L' 'L' 'D']\n",
      " ['U' 'U' 'U' 'D']\n",
      " ['U' 'U' 'R' 'D']\n",
      " ['U' 'R' 'R' 'T']]\n"
     ]
    }
   ],
   "source": [
    "# value iteration\n",
    "\n",
    "theta = 0.0001\n",
    "maxiters = 10\n",
    "#V = np.zeros((rows_count,columns_count)) ## Use arrayV0\n",
    "directions = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "\"\"\"value iteration algorithm\"\"\"\n",
    "\n",
    "while k < maxiters:\n",
    "    delta = 0\n",
    "    k += 1 \n",
    "    with np.printoptions(precision=3):\n",
    "        print(\"k == \", k, \": value func == \\n \\n\", V0)\n",
    "    \n",
    "    for r in range(rows_count):\n",
    "        for c in range(columns_count):\n",
    "            if not isTerminal(r,c, rows_count, columns_count):  ## for all regular states\n",
    "            \n",
    "                oldVal = V0[r,c]\n",
    "                tempList = [getValue(r-1, c, V0) + rewards[0], \n",
    "                             getValue(r, c+1, V0) + rewards[1], \n",
    "                             getValue(r+1, c, V0) + rewards[2], \n",
    "                             getValue(r, c-1, V0) + rewards[3]]\n",
    "                maxVal = max(tempList)\n",
    "                for i in range(len(tempList)):\n",
    "                    if maxVal == tempList[i]:\n",
    "                        maxInd = i\n",
    "                        break\n",
    "                newVal = tempList[maxInd]\n",
    "                V0[r,c] = newVal\n",
    "                delta = ( max( delta, abs( oldVal - newVal ) ) )              \n",
    "\n",
    "            else:\n",
    "                continue ## this is probably bad coding practice, but I couldnt think of any other good solution\n",
    "                         ## to prevent the iteration over the terminal states and break clause problems...\n",
    "    if delta < theta:    ## actually it looks like the break checking delta < theta onlyy happens once per k iterations\n",
    "        k = (maxiters + 100)  ## after each of the states (after the two-nested loop) has been iterated already, \n",
    "                              ## this appears to be good!\n",
    "    \n",
    "        \n",
    "\"\"\"get the deterministic policy afterwards\"\"\"\n",
    "\n",
    "for r in range(rows_count):\n",
    "    for c in range(columns_count):\n",
    "        if not isTerminal(r, c, rows_count, columns_count):\n",
    "            tempList = [getValue(r-1, c, V0) + rewards[0], \n",
    "                        getValue(r, c+1, V0) + rewards[1], \n",
    "                        getValue(r+1, c, V0) + rewards[2], \n",
    "                        getValue(r, c-1, V0) + rewards[3]]\n",
    "            tempMax = max(tempList)    ##get best value from newStates\n",
    "            for n in range(len(tempList)): ## find what index it was\n",
    "                if tempMax == tempList[n]:\n",
    "                    maxInd = n\n",
    "                    break\n",
    "                        \n",
    "            maxChar = actionStringDict[maxInd] ##use the index to access into correct string from the dict\n",
    "            newAct = maxChar\n",
    "            policies[r, c] = newAct\n",
    "\n",
    "            \n",
    "print(\"\\n\\n\\n ending results were: \\n\")\n",
    "with np.printoptions(precision=3):\n",
    "    print(\"value function was \\n\", V0)\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print(\"policy was \\n\", policies)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looks like the value iteration algorithm worked at least for regular case\n",
    "* regular case with rewards all of them == -1\n",
    "* ending policy was an optimal one at the end\n",
    "* this algorithm was very confusing once again, judging from the book chapter, and book examples (in fact there wasnt even a practical example at all, I think)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's check the modified case with value iteration\n",
    "\n",
    "* adjust the up reward to be = -4\n",
    "* other rewards = -1\n",
    "\n",
    "* hope for the best, fingers crossed!!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseting V0, V1 and policies, and initalize rewards \n",
      "\n",
      "[['T' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'A']\n",
      " ['A' 'A' 'A' 'T']]\n",
      "\n",
      "V0 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "V1 [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "rewards \n",
      " [-4, -1, -1, -1]\n",
      "k ==  0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"reset variables for preparation for the value iteration algorithm\"\"\"\n",
    "\n",
    "       \n",
    "        ## reset for new attemp at ex3\n",
    "print('reseting V0, V1 and policies, and initalize rewards \\n')\n",
    "\n",
    "policies = np.array([ ['T','A','A','A'], \n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','A'],\n",
    "                     ['A','A','A','T'] ])\n",
    "print(policies)\n",
    "            \n",
    "print('')\n",
    "V0 = np.zeros((rows_count, columns_count))    #reset the arrays\n",
    "V1 = np.zeros((rows_count, columns_count))\n",
    "\n",
    "\n",
    "with np.printoptions(precision=3):\n",
    "    print('V0',V0)\n",
    "print('')\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print('V1',V1)\n",
    "print('')\n",
    "\n",
    "rewards = [-4, -1, -1, -1] ## must be in order up,right,down,left\n",
    "print('rewards \\n',rewards)\n",
    "k = 0\n",
    "print('k == ',k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k ==  1 : value func == \n",
      " \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "k ==  2 : value func == \n",
      " \n",
      " [[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "k ==  3 : value func == \n",
      " \n",
      " [[ 0. -1. -2. -2.]\n",
      " [-2. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "k ==  4 : value func == \n",
      " \n",
      " [[ 0. -1. -2. -3.]\n",
      " [-3. -3. -3. -2.]\n",
      " [-3. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "k ==  5 : value func == \n",
      " \n",
      " [[ 0. -1. -2. -3.]\n",
      " [-4. -4. -3. -2.]\n",
      " [-4. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "\n",
      " ending results were: \n",
      "\n",
      "value function was \n",
      " [[ 0. -1. -2. -3.]\n",
      " [-4. -4. -3. -2.]\n",
      " [-4. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "policy was \n",
      " [['T' 'L' 'L' 'D']\n",
      " ['U' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'T']]\n"
     ]
    }
   ],
   "source": [
    "# value iteration\n",
    "\n",
    "theta = 0.0001\n",
    "maxiters = 50\n",
    "#V = np.zeros((rows_count,columns_count)) ## Use arrayV0\n",
    "directions = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "\"\"\"value iteration algorithm\"\"\"\n",
    "\n",
    "while k < maxiters:\n",
    "    delta = 0\n",
    "    k += 1 \n",
    "    with np.printoptions(precision=3):\n",
    "        print(\"k == \", k, \": value func == \\n \\n\", V0)\n",
    "    \n",
    "    for r in range(rows_count):\n",
    "        for c in range(columns_count):\n",
    "            if not isTerminal(r,c, rows_count, columns_count):  ## for all regular states\n",
    "            \n",
    "                oldVal = V0[r,c]\n",
    "                tempList = [getValue(r-1, c, V0) + rewards[0], \n",
    "                             getValue(r, c+1, V0) + rewards[1], \n",
    "                             getValue(r+1, c, V0) + rewards[2], \n",
    "                             getValue(r, c-1, V0) + rewards[3]]\n",
    "                maxVal = max(tempList)\n",
    "                for i in range(len(tempList)):\n",
    "                    if maxVal == tempList[i]:\n",
    "                        maxInd = i\n",
    "                        break\n",
    "                newVal = tempList[maxInd]\n",
    "                V0[r,c] = newVal\n",
    "                delta = ( max( delta, abs( oldVal - newVal ) ) )\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "                                 \n",
    "    if delta < theta:  ## it appears that the break check delta < theta happens only after each k iterations, this is good,\n",
    "        k = (maxiters + 100) ## because it appears that we will only check after all states are already iterated over,\n",
    "                              ## after the two-nested loop has been run.... good!!!\n",
    "                               ## these loops are difficult to code in python because of there is only whitespace\n",
    "                                ## to guide our thinking and loop nesting...\n",
    "        \n",
    "\"\"\"get the deterministic policy afterwards\"\"\"\n",
    "\n",
    "for r in range(rows_count):\n",
    "    for c in range(columns_count):  ##iterate over all the actions  in policyTable\n",
    "        if not isTerminal(r, c, rows_count, columns_count): ## in order to find the deterministic policy\n",
    "            tempList = [getValue(r-1, c, V0) + rewards[0], \n",
    "                        getValue(r, c+1, V0) + rewards[1], \n",
    "                        getValue(r+1, c, V0) + rewards[2], \n",
    "                        getValue(r, c-1, V0) + rewards[3]]\n",
    "            tempMax = max(tempList)    ##get best value from newStates\n",
    "            for n in range(len(tempList)): ## find what index it was\n",
    "                if tempMax == tempList[n]:\n",
    "                    maxInd = n\n",
    "                    break\n",
    "                        \n",
    "            maxChar = actionStringDict[maxInd] ##use the index to access into correct string from the dict\n",
    "            newAct = maxChar\n",
    "            policies[r, c] = newAct\n",
    "            \n",
    "print(\"\\n\\n\\n ending results were: \\n\")\n",
    "with np.printoptions(precision=3):\n",
    "    print(\"value function was \\n\", V0)\n",
    "    \n",
    "with np.printoptions(precision=3):\n",
    "    print(\"policy was \\n\", policies)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## it appears that the value iteration algorithm produced reasonable results\n",
    "\n",
    "* I had to debug this value iteration inside SPYDER IDE with python debugger just to be sure about the break check delta < theta just to be sure\n",
    "\n",
    "\n",
    "* it appears that you check delta < theta once per k iterations, such that you first will have already iterated over all the states, in the two-nested loop\n",
    "\n",
    "\n",
    "* both policies the regular case, as well as the case of up reward = -4 appear to be optimal\n",
    "\n",
    "\n",
    "* value iteration algorithm was more difficult to understand than the dynamic programming case with the other algorithm, policyEval + GreedyOpt\n",
    "\n",
    "\n",
    "* however it appears that the value iteration algorithm is much superior in efficiency in convergence speed!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
